---
title: 'Mastering Logistic Regression'
pubDate: 2023-06-16
description: 'A complete and comprehensive guide to understanding logistic regression.'
author: 'Michael Liou'
slug: 'influence'
tags:
  - statistics
toc: true
comments: false
draft: true
---

import LogisticFunction from "../../components/blog-5/LogisticFunction.vue"
// import LogisticRegression from "../../components/sandbox/LogisticRegression.vue"
// import BinaryLikelihood from "../../components/sandbox/BinaryLikelihood.vue"
// import BetaLikelihood from "../../components/sandbox/BetaLikelihood.vue"
// import LogisticLikelihoodLinked from "../../components/sandbox/LogisticLikelihoodLinked.vue"
// import TestMath from "../../components/sandbox/TestMath.astro"
import TestMojs from "../../components/sandbox/TestMojs.vue"

## Introduction

At it's core, a logistic regression is nothing more than fitting a special function to data that takes the values between 0 and 1. The most common use cases are when the response of interest is a *dichotomous* variable, meaning that the variable we're modeling can take either of exactly 2 values. For example, a coin flip can be modeled as a dichotomous variable, where heads is 1 and tails is 0. Whether someone had breakfast or not today is also dichotomous. Sometimes, dichotomous variables are created from continuous variables. For example, high blood pressure can be modeled as dichotomous by creating a cutoff value for what is considered high blood pressure ($\geq 130$ systolic mm Hg). The "regression" part of the name comes in when you want to associate the response with some explanatory variables. For example, if you want to know the relationship between the number of hours you study (explanatory) and passing/failing an exam (response), we can use a logistic regression to model the strength of that relationship.

Logistic regression has found use across many disciplines, including for classification in machine learning models, ecological models of constrained growth, and estimation in retrospective studies, so it's useful to understand the method from first statistical principles in order to fully appreciate the utility and limitations of this method.

### The Logistic Function

The magic of this method starts with the shape of the function itself, with a specific parameterization[^1] for regression purposes. Since this function is the core of the method, we'll spend some time developing some intuition for how this function behaves.

[^1]: Note there are other related parameterizations floating around, but the one I give is most common in statistics because the parameters are linearly related with each other. This allows the function to fit into the framework of Generalized Linear Model theory.

$$
\LARGE{p(x) = \frac{1}{1 + e^{-({\color{#ebcb8b}\beta_0} + {\color{#bf616a}\beta_1} x)}}}
$$

<LogisticFunction client:only="vue" />





### Interpretation of coefficients

Having a function that maps values between 0 and 1 sets the stage perfectly for introducing probability, which will also take values between 0 and 1! To make this into a concrete example, suppose I'm interested in modeling how many fruit snacks one can eat (explanatory variable, $x$) before getting a stomach ache (response variable, $y$). It's reasonable to think about the probability of such an event since I could probably have 100 fruit snacks one day and not get a stomach ache while another day also having 100 and getting a stomach ache. 

{/* <TestMojs client:only="vue"/> */}

So we have a function that maps to a value between 0 and 1 -- this sets the stage perfectly for introducing probability! It's no accident I chose to write the logistic function as $p(x)$ with $p$ standing for probability. The main idea is that we can take any number from the real line, and associate it with a probability of an event happening. Since we think about dichotomous event, things either happen or they don't, the statistical foundation for this is called a Bernoulli distribution. Suppose we model the events we observe with the random variable $Y$, we would say our event follows a Bernoulli distribution with parameter $p$, capturing the probability of the event happening ($Y = 1$). In symbols,

$$
Y \sim \text{Bernoulli}(p)
$$

## Fitting the Model

### The Likelihood Principle

### Likelihood for Regression

### Optimization techniques

## Hypothesis Testing of Coefficients

### The Wald Test

### The Likelihood Ratio Test

### The Score Test

## Model Validation

### Residuals (Working, Deviance, Response, Raw)

### Performance Metrics

## Additional Topics

### Quasi-Binomial Regression

### Relation to other Binary Response Models

### Relations in Categorical Data Analysis

### Multinomial/Ordinal Logistic Regression

### Propensity Score Matching


A logistic regression models values between 0 and 1, but unfortunately there is no closed form solution that one can use to solve for the coefficients in the model. 

The equation for a logistic function is given by:

{/* <LogisticRegression client:only="vue" /> */}

## The Likelihood Principle
{/* <BinaryLikelihood client:only="vue" /> */}

In order to develop a better intuition for


{/* <BetaLikelihood client:only="vue" /> */}

## Likelihood for Regression
{/* <LogisticLikelihoodLinked client:only="vue" /> */}
{/* <TestMath /> */}