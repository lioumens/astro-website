---
title: 'An Interactive Guide to the Bivariate Gaussian Distribution'
pubDate: 2023-07-05
description: "Exploring Mathbox for visualizations"
author: 'Michael Liou'
slug: 'mathbox'
tags:
  - statistics
  - devlog
toc: false
comments: false
draft: true
---

import MathBoxParent from "../../components/sandbox/MathBoxParent.vue";
import MultivariateConditionalParent from "../../components/blog-6/MultivariateConditionalParent.vue";
import MultivariateMarginalParent from "../../components/blog-6/MultivariateMarginalParent.vue";
import BivariateCombinationParent from "../../components/blog-6/BivariateCombinationParent.vue";
import CovarianceEllipse from "../../components/blog-6/CovarianceEllipse.vue";
// import MathBoxAgain from "../../components/sandbox/MathBoxAgain.astro";

## Introduction

It's not long after working with the normal distribution in one dimension that you realize you need to start working with vectors and matrices. Data rarely comes in isolation and we need more statistical tools to work with multiple variables at a time. The bivariate Gaussian distribution is normally the first step into adding more dimensions, and the intuition that one builds from the 2D case is useful in higher dimensions with the multivariate Gaussian.

## Density of the Bivariate Normal

It's a magical mathematical bump. It was a bump in 1 dimension and it's still just a symmetric bump in 2 dimensions. The difference now is that there are more shapes that this hill can have because there's more room to breathe in 3 dimensions! At first glance, there's nothing glamourous about this function but it hides a lot of mathematical beauty.

Recall that the 1d Gaussian distribution has pdf:

$$
f(x;\mu_x, \sigma_x) = \frac{1}{\sigma_x\sqrt{2\pi}} \exp{\left[-\frac{1}{2}\left(\frac{x-\mu_x}{\sigma_x}\right)^2\right]}
$$

where $\sigma_x$ controlled the width of the distribution while $\mu_x$ controlled the location.

The story is similar with the bivariate normal distribution, which is defined with 5 parameters, $\mu_x, \mu_y$ manage the mean of the distribution while $\sigma_x,\sigma_y, \rho$ manage the covariance of the distribution. We say that the pair of random variables $X,Y$ are jointly distributed as a bivariate normal distribution.

If you examine the density function, you can pick out the parts that that it shares with the 1d Gaussian. The interesting parts of this distribution lie in the the new parameter $\rho$, which is the correlation between the two variables, and controls how much one variable moves with the other.


<MathBoxParent client:only="vue" />

Here are some things to notice as you play with the function above:

* The maximum and center of mass of the density is at $(\mu_x, \mu_y)$.
* The mean parameters $\mu_x, \mu_y$ can move around independently of the overall shape of the density.
* $\sigma_x, \sigma_y > 0$, and $\rho \in (-1,1)$. The density function becomes degenerate when you get to the edges of this range. (I limit this range, but hopefully just enough to still get the sense of what happens if you push them to the extremes. Don't break my applet!)
* The contours of this function, no matter the values of the parameters are elliptical.
* When $\rho = 0$, the density is symmetric about the $x$ and $y$ axes.
* When $\rho > 0$, the density seems to form a ridge along a positively sloped line, and when $\rho < 0$ it's along a line of negative slope. Slopes look symmetric along this line. This ridge line is the major axis of the contour ellipses.
* The ridge line is closer to the $x$-axis when $\sigma_x > \sigma_y$, and closer to the $y$-axis when $\sigma_y > \sigma_x$.
* When $\sigma_x = \sigma_y$, as $\rho$ ranges from -1 to 1, the ridge line can range from $-45\degree$ to $45\degree$ (from the $y$-axis). When the ratio of $\sigma_x$ to $\sigma_y$ is more extreme, the power of $\rho$ to change the slope of the ridge line is much more limited.

You should get the sense that the density of the bivariate normal is fundamentally about ellipses. It's ellipses all the way down! This is not a coincidence -- the shape of that ellipse is entirely controlled by the covariance matrix. If we rewrite the density function in vector/matrix terms so that the covariance matrix $\operatorname{Var}\left[\begin{smallmatrix}X \\ Y\end{smallmatrix}\right] = \Sigma$ shows up explicitly, we get:

$$
f(\mathbf{z};\mu_z, \Sigma) = \frac{1}{2\pi}|\Sigma|^{-1/2}\exp \left(-\frac{(\mathbf{z} - \bm{\mu}_{\mathbf{z}})'\Sigma^{-1}(\mathbf{z} - \bm{\mu}_{\mathbf{z}})}{2}\right)
$$

where $\mathbf{z} = \left[\begin{smallmatrix}x \\ y\end{smallmatrix}\right]$ and $\bm{\mu}_\mathbf{z} = \left[\begin{smallmatrix}\mu_x \\ \mu_y\end{smallmatrix}\right]$

The kernel of the function is $(\mathbf{z} - \bm{\mu}_{\mathbf{z}})'\Sigma^{-1}(\mathbf{z} - \bm{\mu}_{\mathbf{z}})$, and has a special name. It is the Mahalonobis distance (squared) between $z$ and the bivariate normal distribution, and the function responsible for the elliptical contours.

### The Covariance Ellipse

The major components of drawing an ellispe are the center, and two diameters. One for the *major* axis and one for the *minor* axis. The major axis goes through the center and is the longest line you can draw, the minor axis is perpendicular to that, also through the center. The covariance matrix $\Sigma$ has all the information you need to draw the ellipse if you make the center of the ellipse at $\bm{\mu}_\mathbf{z}$. The radii are the square roots of the eigenvalues of $\Sigma$. The eigenvectors of $\Sigma$ give the directions of the major and minor axes.

<CovarianceEllipse client:only="vue" />


## Conditional Distribution

The bivariate distribution has many beautiful symmetries, and I think they are highlighted well by the conditional distribution. The formula for the conditional distribution is a little more complicated at first but I think the picture and mental model is much more intuitive. Given that $(X, Y)$ are jointly bivariate normal, the conditional distribution $X | Y=y$ is interpreted as "I know the value of $Y$ to be $y$, what is the variability and probability distribution left in the $X$ direction." Since these random variables had a joint distribution, knowing the value of one of them should influence the information I know about the other one. 

If I know that $Y=2$, I'm thinking about slicing my density function along the plane of $y=2$. Once I take the slice, we will need a scaling factor in order to make sure the slice integrates to 1 for a valid probability distribution.

<MultivariateConditionalParent client:only="vue" />

Here are some things to notice:

* The conditional distribution is a 1d Gaussian distribution.
* The conditional mean is a function fo $y$ only when $\rho \neq 0$. Since $\rho$ controls the "shearness" of the ellipse, the conditional mean shifts more
* Even when $\rho \neq 0$, the conditional variance is not affected by $\sigma_y$.

## Marginal Distribution

<MultivariateMarginalParent client:only="vue" />

## Linear Combinations

<BivariateCombinationParent client:only="vue" />

{/* <MathBoxAgain /> */}